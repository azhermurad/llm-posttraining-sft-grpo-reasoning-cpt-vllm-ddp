# Base configuration for all training stages
model:
  base_model: "unsloth/Qwen3-8B"
  max_seq_length: 2048 # Choose any! We auto support RoPE Scaling internally!
  dtype: None  # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+
  load_in_4bit: True  # Use 4bit quantization to reduce memory usage. Can be False 16 Bit Default.

# Add LoRA adapters so we only need to update 1 to 10% of all parameters! 
# We also add embed_tokens and lm_head to allow the model to learn out of distribution data.

lora:
  r: 128   # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128
  lora_alpha: 32
  lora_dropout: 0  # Supports any, but = 0 is optimized
  bias: "none"  # Supports any, but = "none" is optimized
    # [NEW] "unsloth" uses 30% less VRAM, fits 2x larger batch sizes!
  target_modules:
    - "q_proj"
    - "k_proj"
    - "v_proj"
    - "o_proj"
    - "gate_proj"
    - "up_proj"
    - "down_proj"
    - "embed_tokens" 
    - "lm_head"

   # [NEW] "unsloth" uses 30% less VRAM, fits 2x larger batch sizes!
  use_gradient_checkpointing: "unsloth" # True or "unsloth" for very long context
  random_state: 34077
  use_rslora: True   # We support rank stabilized LoRA
  loftq_config: null # And LoftQ

training:
  per_device_train_batch_size: 2
  gradient_accumulation_steps: 4

  warmup_steps: 10
  max_steps: 30
  # warmup_ratio = 0.1,
  # num_train_epochs: 1


    # Select a 2 to 10x smaller learning rate for the embedding matrices!
  learning_rate: 5e-5
  embedding_learning_rate: 1e-5

  save_steps: 100
  save_total_limit: 2

  logging_steps: 1
  optim: "adamw_8bit"
  weight_decay: 0.001
  lr_scheduler_type: "linear"
  seed: 3407
  report_to: "wandb" # Use TrackIO/WandB etc

 

dataset:
  name: "wikipedia"
  config: "20231101.ur"
  split: "train[:1%]"

wandb:
  project: ""

paths:
  output_dir: "Aqal-1.0-8B-Base-lora"
  checkpoint_dir: "./checkpoints"
  final_model_local_save_dir_name: "Aqal-1.0-8B-Base" # Aql‑1.0‑7B‑Base
  final_model_hub_save_dir_name: "Aqal-1.0-8B-Base" # Aql‑1.0‑7B‑Base